---
title: "Prediction of Health Insurance Costs with Linear Regression"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![](https://www.meaningfulwomen.com/wp-content/uploads/Why-Having-Health-Insurance-Is-Overall-A-Good-Thing-678x381.jpg)


**Motivation:** Health insurance is a type of insurance that covers the whole or a part of the risk of a person incurring medical expenses. Health insurance companies desire to provide appropriate insurance for each person such that it may enhance the resources allocation of the company and may increase the loans they give. 

**Goal:** Build a regression model which predicts health expenses for potential insured based on their personal information. 

**Data:** Tabular data with 1,338 records. Each record consists of personal information about a potential insured. There are 7 explainable variables: age, sex, bmi, smoker/nonsmoker, region, number of children and family size. The response vector is the health expenses (in dolars) of each potential insured. The data is obtained from the U.S only.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
library("ggplot2")
library("GGally")
library("plyr")
#install.packages("memisc")
library(memisc)
library(caTools)
library(olsrr)
library("broom")
library(SciViews)

```

```{r data,  echo=FALSE,  message = FALSE, warning = FALSE}
# Load data
#setwd("./Desktop/")
data  <-  read.csv("insurance.csv", header=TRUE)
head(data)
```

### **Explanatory Analysis**

#### **Variables Distribution**



```{r, fig.show="hold", out.width="30%", fig.ncol = 3, echo=FALSE,  message = FALSE, warning = FALSE}

pie_chart <- function(column, title) {
  temp = count(column)
  ggplot(temp, aes(x="", y=freq, fill=x)) +
  geom_bar(stat="identity", width=1, color="white") +
  coord_polar("y", start=0) +
  theme_void()+ # remove background, grid, numeric labels
  geom_text(aes(y = freq/length(freq) + c(0, cumsum(freq)[-length(freq)]),
                label = paste0(round(freq*100/sum(freq)), "%")), size=5)+
  guides(fill=guide_legend(title=title))
  
}

pie_chart(data$sex, "sex")
pie_chart(data$smoker, "smoker")
pie_chart(data$region, "region")


bin_chart <-function(data, column, column_name) {
  ggplot(data, aes(factor(column))) +
  geom_bar(aes(y = (..count..)/sum(..count..))) + 
  scale_y_continuous(labels=scales::percent) +
  xlab(column_name)+
  ylab("")+
  theme_minimal()+
  scale_fill_brewer(palette="Blues")+
  theme(legend.position = "none")
}

bin_chart(data, data$children, "children")
bin_chart(data, data$family_size, "family_size")

density_chart <- function(data, column, column_name) {
  ggplot(data, aes(x = column)) +
  geom_density(alpha = .2, fill="#FF6655")+
  xlab(column_name)
}

density_chart(data, data$age, "age")


density_chart <-function(data, column, column_name) {
  ggplot(data, aes(column, fill = cut(column, 100))) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), show.legend = FALSE) +
  scale_y_continuous(labels=scales::percent) +
  theme_minimal() +
  labs(x = column_name, y = "") +
  scale_fill_discrete(h = c(180, 360), c = 150, l = 80)
}

density_chart(data, data$age, "age")
density_chart(data, data$bmi, "bmi")
density_chart(data, data$expenses, "expenses")


#ggplot(data, aes(x = age, fill = sex)) +
#geom_histogram(binwidth = .5, alpha =.8, position = "dodge")

```


#### **Scatter plots for continues variables and semi-continues variables (age)**
```{r, echo=FALSE,  message = FALSE, warning = FALSE}
# Scatter plots  for continues variables
ggpairs(data[c("age", "bmi", "expenses")], colour="blue")
```

#### **Box plots**

```{r, fig.show="hold", out.width="50%", echo=FALSE,  message = FALSE, warning = FALSE}
ggplot(data, aes(x=sex, y=expenses, group=sex, fill=sex)) +
geom_boxplot() +
theme(axis.text.x = element_blank())


ggplot(data, aes(x=smoker, y=expenses, group=smoker, fill=smoker)) +
geom_boxplot() +
theme(axis.text.x = element_blank())


```

It can be seen that smokers have higer health expensess.


```{r, fig.show="hold", out.width="50%", echo=FALSE,  message = FALSE, warning = FALSE}
ggplot(data, aes(x=children, y=expenses, group=children, fill=children)) +
geom_boxplot() +
scale_x_discrete(limits=factor(1:5))

ggplot(data, aes(x=family_size, y=expenses, group=family_size, fill=family_size))+
geom_boxplot() +
scale_x_discrete(limits=factor(1:8))

```


Interesting to see that the expensess is getting higer untill family_size=5, and then start to deacreasing. mabe because there are not many big families in the dataset.

```{r, echo=FALSE,  message = FALSE, warning = FALSE}
ggplot(data, aes(x=region, y=expenses, group=region, fill=region)) +
geom_boxplot() +
theme(axis.text.x = element_blank(),
      axis.text.y = element_blank())

ggplot(data, aes(x=smoker, y=expenses, group=smoker, fill=smoker)) +
geom_boxplot() +
facet_wrap(~ region, nrow = 2)

```

## Scatter Plots
```{r, echo=FALSE,  message = FALSE, warning = FALSE}
ggplot(data, aes(x=family_size, y=children)) +
geom_point() 

ggplot(data, aes(x=family_size, y=age)) +
geom_point() +
geom_jitter(width = 0, height = 0.5)


```

```{r, fig.show="hold", out.width="50%", echo=FALSE,  message = FALSE, warning = FALSE}

ggplot(data, aes(x=age, y=expenses, color=smoker)) +
geom_point() 

ggplot(data, aes(x=age, y=expenses, color=sex)) +
geom_point() 

ggplot(data, aes(x=bmi, y=expenses, color=smoker)) +
geom_point() 

ggplot(data, aes(x=bmi, y=expenses, color=sex)) +
geom_point() 


```
We can see from the figure "age vs expensess" that as the age getting higer also does the expeness. In addition we can see that non-smokers have low expensess compare to smokers, and that  are 2-order polynomial patterns for that in the plot. Therefore we may benfit from add an interaction "'smoker*age", as well as add the variable $age^2$ to the model.

We can see from the figure "bmi vs expensess", that an interaction "smoker*age" may be relavnt too. In addition, BMI in the range 18.4-24 considerd as normal, while BMI over 30 is considerd as unhealty condition. So we may benfit from add to the model an incdicator "bmi30" which indicated whatever a indvidual has BMI of 30 or above

## More plots
```{r, fig.show="hold", out.width="50%", echo=FALSE,  message = FALSE, warning = FALSE}


ggplot(data, aes(x = expenses)) +
geom_density(alpha = .2, fill="#FF6655") +
labs(x = "Expenses", y = "Density", title = "Expenses distribution by Sex") +
facet_wrap(~ sex, nrow = 2)

ggplot(data, aes(x = expenses)) +
geom_density(alpha = .2, fill="#FF6655") +
labs(x = "Expenses", y = "Density", title = "Expenses distribution by Smoker") +
facet_wrap(~ smoker, nrow = 2)

ggplot(data, aes(x = expenses)) +
geom_density(alpha = .2, fill="#FF6655") +
labs(x = "Expenses", y = "Density", title = "Expenses distribution by Region") +
facet_wrap(~ region, nrow = 2)

ggplot(data, aes(x = expenses)) +
geom_density(alpha = .2, fill="#FF6655") +
labs(x = "Expenses", y = "Density", title = "Expenses distribution by family_size") +
facet_wrap(~ family_size, nrow = 2)
```


## Check for Multicollinearity

```{r, echo=FALSE,  message = FALSE, warning = FALSE}

model = lm(expenses ~ age + sex + bmi + children + smoker + region + family_size, data = data)
ols_coll_diag(model)

```

It can be seen that "children" and "family_size" are colinear with each other (as expected). We can see it because they both have high VIF (~9), and becausein the row of condition index with high value (26.26), thier variance proportions are close to 0.9. So we drop "children" and reapt multicollinearity checking without it:

```{r, echo=FALSE,  message = FALSE, warning = FALSE}

model = lm(expenses ~ age + sex + bmi + smoker + region + family_size, data = data)
ols_coll_diag(model)

```
It seems there are no other variables that colinear with each other, so we remain these features.

###  **Basic Model**

Now we build a baseline model which consists only from the variables "age" and "smoker". we chosed these variables because it seems from the plots that both are important.

In order to build the model properly, we e split the data into 75% train set and 25% test set.We will use the train set foe model development, and we will use the test set for evaluate the final model.


```{r, echo=FALSE,  message = FALSE, warning = FALSE}
# Split to Train/Test sets
set.seed(42) # Set Seed so that same sample can be reproduced in future also
sample = sample.split(data$expenses, SplitRatio = .75)
train = subset(data, sample == TRUE)
test  = subset(data, sample == FALSE)

row.names(train) <- NULL # rest index 
row.names(test) <- NULL # rest index 


```

```{r, message = FALSE, warning = FALSE}
basic_model = lm(expenses ~ age + smoker , data = train)
summary(basic_model)
```
#### **Residuals Analysis**

Before we look at the statistical measures for Model Quality, we should check the residual plots in order to verify the assumptions of an OLS linear regression model:

1. The residuals have to be normally distributed
2. The error terms need to be independent of each other (need to be random)

If some assumptions are violated in the fitted model, we risk producing results (regression coefficients and other numeric results) that we can’t trust.

Residual plots display the residual values on the y-axis and fitted values, or another variable, on the x-axis, and it can be used to display unwanted patternss that incdicates of assumption violation.


**Check the assumption that the residuals are normaly distributed**


```{r, fig.show="hold", out.width="50%", echo=FALSE,  message = FALSE, warning = FALSE}

# Resuduls histogram for basic_model
ggplot(data = train, aes(x = basic_model$residuals)) +
geom_histogram(fill = 'steelblue', color = 'black') +
labs(title = 'Histogram of Residuals', x = 'Residuals', y = 'Frequency')

# QQ-plot for basic_model
qqnorm(basic_model$residuals, pch = 1, frame = FALSE)
qqline(basic_model$residuals, col = "steelblue", lwd = 2)

```

We can see from the residuals histogram and from the QQ-plot that the residuals distributon is right-skewd and has long right tail - meaning the residuals arn't normally distubuted. We can try to fix this by transforming the resposne / explainable vaiables or by adding features.

**Check the assumption that the residuals are randomly dispersed around the 0 horizontal axis**

The idea is that the gap between the expected and observed values must not be predictable. Or, no explanatory power should be in the error. If it can use the error to make predictions about the response, the model has a problem.

The theory here is that the deterministic component of a regression model does such a great job of explaining the dependent variable that it leaves only the intrinsically inexplicable portion of your study area for the error. If it can identify non-randomness in the error term, the independent variables are not explaining everything that they can.



```{r, fig.show="hold", out.width="33%",fig.ncol = 3, echo=FALSE,  message = FALSE, warning = FALSE}

plot(basic_model, which=1)


ggplot(train, aes(x=age, y=basic_model$residuals)) +
geom_point()

ggplot(train, aes(x=smoker, y=basic_model$residuals)) +
geom_point() 


```

We can see that the residuals (y-Axis) are randomly scattered around zero for the entire range of fitted values. When the residuals center on zero, they indicate that the model’s predictions are correct on average rather than systematically too high or low.

If we add color by "smoker" for the middle plot we get:

```{r, fig.show="hold", echo=FALSE,  message = FALSE, warning = FALSE}

ggplot(train, aes(x=age, y=basic_model$residuals, color=smoker)) +
geom_point()


```

We can see that almost all the points that close to y=0 are points that belongs to non-smokers. We can take advantage of this information and add interaction smoker*age to the improved model.

**Residuals vs variables that arn't used in the basic model**

We will color the points by "smoker".

```{r, fig.show="hold", out.width="33%",fig.ncol = 3, echo=FALSE,  message = FALSE, warning = FALSE}

ggplot(train, aes(x=bmi, y=basic_model$residuals, color=smoker)) +
geom_point()

ggplot(train, aes(x=sex, y=basic_model$residuals, color=smoker)) +
geom_point() 

ggplot(train, aes(x=region, y=basic_model$residuals, color=smoker)) +
geom_point()

ggplot(train, aes(x=family_size, y=basic_model$residuals, color=smoker)) +
geom_point()


```



#### **Model Quality**

Even tho some assumptopn are violated in the basic fitted model, we will calculate some model qualty measures:

**R-squared (R2)** is the proportion of variation in the outcome that is explained by the predictor variables. The Higher the R-squared, the better the model.

**Adjusted R-squared**, adjusts the R2 for having too many variables in the model (in higer dimension it is s easier to find linear subspace that fit the data, but it may lead to strong overfitting!)

**AIC** stands for (Akaike’s Information Criteria), a metric developped by the Japanese Statistician, Hirotugu Akaike, 1970. The basic idea of AIC is to penalize the inclusion of additional variables to a model. It adds a penalty that increases the error when including additional terms. The lower the AIC, the better the model.

**BIC** (or Bayesian information criteria) is a variant of AIC with a stronger penalty for including additional variables to the model.

**Mallows Cp:** A variant of AIC developed by Colin Mallows.

```{r, echo=FALSE,  message = FALSE, warning = FALSE}
glance(basic_model) %>% dplyr::select(r.squared, adj.r.squared, AIC, BIC)

full_model = lm(expenses ~ . -X , data = train)
paste0("Mallows C_p: ", round(ols_mallows_cp(basic_model, full_model), 2))
```

We got $R^2=0.71, Adjusted R^2=0.71$, which is relativly high scores. 

#### **Influential Observations**


```{r, fig.show="hold", out.width="50%",fig.ncol = 2, echo=FALSE,  message = FALSE, warning = FALSE}

# Cook's Distance
n = nrow(train)
p=length(basic_model$coefficients)-1
cutoff <- 4/(n-p-2)
plot(basic_model, which=4, cook.levels=cutoff)
abline(h=cutoff, lty=2, col="red")

plot(basic_model, which=5)

library(car) 
influencePlot(basic_model, id.method="identify", main="Influence Plot", sub="Circle size is proportional to Cook’s distance")


```

Let's see what values have the three most influential observations:

```{r, echo=FALSE,  message = FALSE, warning = FALSE}

train[544, ] 
train[578, ] 
train[820, ] 

```

We didn't sucssed to identify why these observation are outliers.

```{r, fig.show="hold", out.width="50%",fig.ncol = 2, echo=FALSE,  message = FALSE, warning = FALSE}

# Calculate DFSBETAS
dfb_df = as.data.frame(dfbeta(basic_model))
dfbs_df = as.data.frame(dfbetas(basic_model))
n = nrow(train)
cutoff = round(2/sqrt(n), 2)

DFSBETAS_plot <- function(dfbs_df, variable) {
  ggplot(dfbs_df, aes(x=1:n ,variable)) + 
  geom_point(shape=18, color="blue")+
  geom_hline(yintercept=cutoff, color="red")+
  geom_hline(yintercept=-cutoff, color="red")+
  annotate("text", x=500, y=cutoff, label=paste("cutoff=", toString(cutoff)))+
  labs(title="DFSBETAS", x="Observations")
}

#DFSBETAS_plot(dfbs_df, dfbs_df$`(Intercept)`)
#DFSBETAS_plot(dfbs_df, dfbs_df$age)
#DFSBETAS_plot(dfbs_df, dfbs_df$smokeryes)

# Calculate DIFFIT
dffits_df = as.data.frame(dffits(basic_model))
p=length(basic_model$coefficients)-1
cutoff_dffit = round(2*sqrt(p+1) / (n-p-1), 2)

DFFITS_plot = ggplot(dffits_df, aes(x=1:n ,`dffits(basic_model)`)) + 
  geom_point(shape=18, color="blue")+
  geom_hline(yintercept=cutoff, color="red")+
  geom_hline(yintercept=-cutoff, color="red")+
  annotate("text", x=500, y=cutoff_dffit, label=paste("cutoff=", toString(cutoff_dffit)))+
  labs(title="DFFITS", x="Observations")

#DFFITS_plot


```


### **Improved model**

Now we build an improved model. 
we add the following variables in addition to "age" and "smoker":

- sex (as dummy variable)
- children (as dummies)
- region (as dummies)
- age^2 (non-linear variable)
- age^2*smoker (interaction)
- bmi30*smoker (interaction)

"family_size" was excluded since its information is included already in "children".

In addition we tried to transform the response with ln/sqrt/^-1 functions but it didn't improve the model, so we left the response as it is.

```{r,  message = FALSE, warning = FALSE}

age2 = (train$age-mean(train$age))^2
bmi30 = ifelse(train$bmi >=30, 1, 0) 

improved_model = lm(expenses ~ age + age2*smoker + factor(children) + bmi + sex + bmi30*smoker + region, data=train)

summary(improved_model)


```

**Check the assumption that the residuals are normaly distributed**


```{r, fig.show="hold", out.width="50%", echo=FALSE,  message = FALSE, warning = FALSE}

# Resuduls histogram for basic_model
ggplot(data = train, aes(x = improved_model$residuals)) +
geom_histogram(fill = 'steelblue', color = 'black') +
labs(title = 'Histogram of Residuals', x = 'Residuals', y = 'Frequency')

plot(improved_model)

# QQ-plot for basic_model
qqnorm(improved_model$residuals, pch = 1, frame = FALSE)
qqline(improved_model$residuals, col = "steelblue", lwd = 2)

```



**Check the assumption that the residuals are randomly dispersed around the 0 horizontal axis**

```{r, fig.show="hold", out.width="33%",fig.ncol = 3, echo=FALSE,  message = FALSE, warning = FALSE}

plot(improved_model, which=1)


ggplot(train, aes(x=improved_model$fitted.values, y=improved_model$residuals, color=smoker)) +
geom_point()

ggplot(train, aes(x=improved_model$fitted.values, y=improved_model$residuals, color=age)) +
geom_point()

ggplot(train, aes(x=improved_model$fitted.values, y=improved_model$residuals, color=age2)) +
geom_point()

ggplot(train, aes(x=improved_model$fitted.values, y=improved_model$residuals, color=factor(bmi30))) +
geom_point()

ggplot(train, aes(x=age, y=improved_model$residuals, color=smoker)) +
geom_point()

ggplot(train, aes(x=smoker, y=improved_model$residuals)) +
geom_point() 

ggplot(train, aes(x=bmi, y=improved_model$residuals, color=smoker)) +
geom_point() 

ggplot(train, aes(x=sex, y=improved_model$residuals, color=smoker)) +
geom_point() 

ggplot(train, aes(x=family_size, y=improved_model$residuals, color=smoker)) +
geom_point() 





```

#### **Influential Observations**


```{r, fig.show="hold", out.width="50%",fig.ncol = 2, echo=FALSE,  message = FALSE, warning = FALSE}

# Cook's Distance
n = nrow(train)
p=length(improved_model$coefficients)-1
cutoff <- 4/(n-p-2)
plot(improved_model, which=4, cook.levels=cutoff)
abline(h=cutoff, lty=2, col="red")

plot(improved_model, which=5)

library(car) 
influencePlot(improved_model, id.method="identify", main="Influence Plot", sub="Circle size is proportional to Cook’s distance")


```

Let's see what values have the three most influential observations:

```{r, echo=FALSE,  message = FALSE, warning = FALSE}


train[763, ] 
train[242, ] 
train[410, ] 
```


#### **Compare models**

Here we compare 3 models:

- basic_model
- improved_model
- improved_model_no_outliers: This is the previous model without the three most influental observation

```{r, echo=FALSE,  message = FALSE, warning = FALSE}
# basic model
print("basic_model")
glance(basic_model) %>% dplyr::select(r.squared, adj.r.squared, AIC, BIC)
paste0("Mallows C_p: ", round(ols_mallows_cp(basic_model, full_model), 2))

# improved model
print("improved_model")
glance(improved_model) %>% dplyr::select(r.squared, adj.r.squared, AIC, BIC)
paste0("Mallows C_p: ", round(ols_mallows_cp(improved_model, full_model), 2))

# improved model no influental observation
print("improved_model_no_outliers")
train_no_outliers <- train[-c(763, 242, 410), ]
bmi2 = (train_no_outliers$bmi-mean(train_no_outliers$bmi))^2
age2 = (train_no_outliers$age-mean(train_no_outliers$age))^2
improved_model_no_outliers = lm(log(expenses) ~ age + smoker + age*smoker + bmi*smoker + sex  +age2 + family_size , data = train_no_outliers)
glance(improved_model_no_outliers) %>% dplyr::select(r.squared, adj.r.squared, AIC, BIC)
paste0("Mallows C_p: ", round(ols_mallows_cp(improved_model_no_outliers, full_model), 2))
```
We get that "improved model" has the best R-squared result (the highest), 
and  that "improved_model_no_ouliers" has the best AIC and BIC results (lowest AIC and BIC). 

#### **Features Selection**

Now we will try 4 different methods for feature selection:

1.	Forward selection <br/>
2.	Backward selection <br/>
3.	Stepwise selection (combination of 1. And 2.) <br/>
4.	bestglm <br/>

We difine the "minimal model" as the model which includes only "smoker", and define the "full model" as the model which includes all variables expect "children" (because it is almost the same as "family_size").


**1. Foward Selection**

```{r, echo=FALSE,  message = FALSE, warning = FALSE}


# specify min and max models 
frmla = 'expenses ~ age + sex + bmi  + smoker + region + family_size'
reg.min = lm(expenses ~ 1, data=train)
reg.full = lm(frmla, data=train)

#output of stepAIC:
#Sum of Sq = change in SSE caused by adding/deleting the variable to the model
#RSS = SSE after the variable is added/deleted

## Forward selection (k is what we called kappa in class. k=2 gives AIC)
reg.fw = stepAIC(reg.min, scope=list(lower=formula(reg.min), 
                                     upper=formula(reg.full)), direction='forward', k=2)



```
**2. Backward selection**



```{r, echo=FALSE,  message = FALSE, warning = FALSE}
# Backwards selection
reg.bw = stepAIC(reg.full, scope=list(lower=formula(reg.min), 
                                      upper=formula(reg.full)), direction='backward', k=2)
```

**3. Stepwise Selection (a combination of 1. And 2.)**


```{r, echo=FALSE,  message = FALSE, warning = FALSE}
# Stepwise selection
reg.bth = stepAIC(reg.min, scope=list(lower=formula(reg.min), 
                                      upper=formula(reg.full)), direction='both', k=2)

```


**4.	bestglm**

```{r, echo=FALSE,  message = FALSE, warning = FALSE}

library("bestglm")
bestAIC <- bestglm(train, IC="AIC")
bestAIC_model <- bestAIC$BestModel


```

```{r, echo=FALSE,  message = FALSE, warning = FALSE}
print("-----Foward:-----")
reg.fw
print("-----Backward:-----")
reg.bw
print("-----Stepwise:-----")
reg.bth
print("-----bestglm:-----")
bestAIC_model
```

It can be seen that the first 3 methods picked the variables "age", "bmi", "smokeryes" and  "family_size", and all methods assign the same coefficients. However the last method picked the same variables and in addition picked "children", and assigned different coefficients than the first three.

### **Final Model**

We choose "improved_model" as the final model.


Lets examine the its results:

```{r,  message = FALSE, warning = FALSE}
summary(improved_model)
```

**F-test:** <br/>
The F-test is used to performs a "joint" hypothesis testing: <br/>
H0: All of the regression coefficients are equal to zero (the model has no predictive capability) <br/>
H1: Otherwise (meaning at least one of the coefficients isn't equal to zero)

Here we get a significant result (very small p-value for the F statistic, in specific lower than significance level of 1%), so we can decide to reject the null hypotesis and conclude that the basic model has some predictive capability.

**T-test(s):** <br/>
Each T-test is used to check the predictive capability of one feature only: <br/>
H0: The coefficient of the i'th feature is equal to zero (there is no linear connection between the outcome and the feature) <br/>
H1: Otherwise

Here  we get significant results for almost all variables, meaning there is an evidence that these variables have linear connection to the oucome.

### **Final Model Evaluaion**

Evaluate model on test set:

```{r, echo=FALSE,  message = FALSE, warning = FALSE}

MSE <- function(y, y_pred) {
  samples_num = nrow(y)
  return ((sum((y-y_pred)**2))/samples_num)
}

#ln(expenses) ~ age + smoker +age*smoker + bmi*smoker + sex  + age2 + family_size

test_X = test[-c(1)] # exclude "expenses"
test_X$age2 = test_X$age^2  # add age2
test_X$bmi30 = ifelse(test_X$bmi >=30, 1, 0) #add bmi30

pred_y <- predict(improved_model, test_X)

print("MSE final model:")
MSE(test["expenses"], pred_y)


# if we transforme the response with ln we should use this formula:
#MSE(ln(test["expenses"]), pred_y)


```



